{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google LLC\n",
    "# Copyright (c) Xuanyi Dong [GitHub D-X-Y], 2020.06 \n",
    "# Copyright 2025 Anonymized Authors\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); \n",
    "# you may not use this file except in compliance with the License. \n",
    "# You may obtain a copy of the License at\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\"\"\"\n",
    "This notebook is designed to familiarize with the code and the underlying \n",
    "datasets packages such as NAS-Bench-101 and NATS-Bench. We will introduce the\n",
    "functions that are used in our nas_utils.py to reduce boilerplate code.\n",
    "\n",
    "Requirements: \n",
    "\n",
    "- the install script should automatically install all libraries\n",
    "-This notebook requires that tensorflow and numpy be installed within the \n",
    "Python environment you are running this script in. \n",
    "- NATS-Bench: \n",
    "git submodule update --init --recursive\n",
    "cd ./thirdparty/autodl\n",
    "pip install .\n",
    "- NAS-Bench-101\n",
    "pip install -i https://test.pypi.org/simple/ nasbench-TF2 (this is a TF2 version)\n",
    "python ./experiments/utils/download_nasbench.py \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we check if or benchmarks are correctly installed and working\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf # required for nasbench\n",
    "import nasbench # checking if this causes any errors\n",
    "from nasbench.api import ModelSpec, NASBench\n",
    "import nats_bench # checking if this causes any errors \n",
    "from nats_bench import create\n",
    "import torch # required for natsbench\n",
    "\n",
    "print(f\"Installed numpy version     : {np.__version__:11s} | expected: 1.26.4\")\n",
    "print(f\"Installed TensorFlow version: {tf.__version__:11s} | expected: 2.15.0\")\n",
    "print(f\"Installed torch version     : {torch.__version__:11s} | expected: 2.3.0\")\n",
    "\n",
    "# check if submodule is installed correctly\n",
    "submodule_installed = (os.path.isfile(os.path.join(\"..\", \"thirdparty\", \"autodl\", \"exps\", \"NATS-algos\", \"regularized_ea.py\")))\n",
    "print(f\"Submodule file found: {submodule_installed}\")\n",
    "\n",
    "\n",
    "# TORCH_HOME needs to be set\n",
    "if \"TORCH_HOME\" not in os.environ:\n",
    "    print(\"TORCH_HOME was not found. Please refer to readme.\")\n",
    "else:\n",
    "    print(\"TORCH installed correctly\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load the nasbench \n",
    "def load_nasbench(path=None):\n",
    "    \"\"\"Loads nasbench dataset from data.\"\"\"\n",
    "    if path is None:\n",
    "        path = os.path.join(\"..\",\"generated\", \"nasbench_only108.tfrecord\") \n",
    "\n",
    "    physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "\n",
    "    if len(physical_devices) > 0:\n",
    "        print(\"Using GPU\")\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    else:\n",
    "        print(\"No GPU.\")\n",
    "\n",
    "    return NASBench(path)\n",
    "\n",
    "# we expect the download script to download the dataset to ../generated/nasbench_only108.tfrecord\n",
    "nasb = load_nasbench() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are helper functions for running our code \n",
    "\n",
    "# defines benchmark specific constants\n",
    "class NASBenchConstants:\n",
    "    INPUT = \"input\"\n",
    "    OUTPUT = \"output\"\n",
    "    CONV3X3 = \"conv3x3-bn-relu\"\n",
    "    CONV1X1 = \"conv1x1-bn-relu\"\n",
    "    MAXPOOL3X3 = \"maxpool3x3\"\n",
    "    NUM_VERTICES = 7\n",
    "    MAX_EDGES = 9\n",
    "    EDGE_SPOTS = NUM_VERTICES * (NUM_VERTICES - 1) / 2  # Upper triangular matrix\n",
    "    OP_SPOTS = NUM_VERTICES - 2  # Input/output vertices are fixed\n",
    "    ALLOWED_OPS = [CONV3X3, CONV1X1, MAXPOOL3X3]\n",
    "    ALLOWED_EDGES = [0, 1]  # Binary adjacency matrix\n",
    "    OPS_IND = list(range(len(ALLOWED_OPS)))\n",
    "    TRIU = np.triu_indices(NUM_VERTICES, k=1)\n",
    "\n",
    "\n",
    "# a wrapper function for the original NAS-Bench-101 encoding that allows to use \n",
    "# its original usecase but you can extend the support for a onehot flattened with\n",
    "# flat = spec.to_flat() and spec = SpecOneHot.spec_from_flat()\n",
    "class SpecOneHot(ModelSpec):\n",
    "    \"\"\"Spec extension to support categorical one-hot for operations and adjacency matrix.\"\"\"\n",
    "\n",
    "    flat_matrix = ((NASBenchConstants.NUM_VERTICES-1+1)*(NASBenchConstants.NUM_VERTICES-1)//2) * 2\n",
    "    input_size = ((NASBenchConstants.NUM_VERTICES-1+1)*(NASBenchConstants.NUM_VERTICES-1)//2)*2 + len(NASBenchConstants.ALLOWED_OPS)*(NASBenchConstants.NUM_VERTICES - 2)\n",
    "  \n",
    "    name_lu = {op: np.eye(3)[i] for i, op in enumerate(NASBenchConstants.ALLOWED_OPS)}\n",
    "    onehot_lu = list(name_lu.values())\n",
    "\n",
    "    def __init__(self, matrix, ops):\n",
    "        super().__init__(matrix=matrix, ops=ops)\n",
    "        # self.flat = self.to_flat() # only when using instead of when initializing\n",
    "\n",
    "    def to_flat(self):\n",
    "        ops = self.original_ops[1:-1] # omit first and last one \n",
    "        mat = self.original_matrix\n",
    "        ops_onehot = np.concatenate([SpecOneHot.name_lu[s] for s in ops])\n",
    "        flat = self.encode(mat, ops_onehot)\n",
    "        return flat.astype(int)\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(matrix,ops_onehot):\n",
    "        # TRIU only uses upper triangle matrix indices including diagonal\n",
    "        onehot_matrix = np.eye(2)[matrix[NASBenchConstants.TRIU]].flatten()\n",
    "        flattened =  np.concatenate((onehot_matrix,ops_onehot))\n",
    "        return flattened\n",
    "    \n",
    "    @staticmethod\n",
    "    def decode(flat):\n",
    "        matrix = np.zeros((NASBenchConstants.NUM_VERTICES, NASBenchConstants.NUM_VERTICES), dtype=flat.dtype)\n",
    "        onehot_matrix = flat[:SpecOneHot.flat_matrix].reshape((-1,2))\n",
    "        entries = np.argmax(onehot_matrix, 1)\n",
    "        matrix[NASBenchConstants.TRIU] = entries\n",
    "        assert np.all(np.triu(matrix) == matrix)\n",
    "\n",
    "        ops_onehot = flat[SpecOneHot.flat_matrix:].reshape((NASBenchConstants.NUM_VERTICES - 2, len(NASBenchConstants.ALLOWED_OPS)))\n",
    "        assert np.sum(ops_onehot) == NASBenchConstants.NUM_VERTICES - 2\n",
    "\n",
    "        indices = np.argmax(ops_onehot, 1)\n",
    "        return matrix, indices\n",
    "\n",
    "    @staticmethod\n",
    "    def test_decode_encode():\n",
    "        rando = np.random.randint(2, size=NASBenchConstants.NUM_VERTICES*NASBenchConstants.NUM_VERTICES)\n",
    "        matrix = rando.reshape((NASBenchConstants.NUM_VERTICES, NASBenchConstants.NUM_VERTICES))\n",
    "        matrix = np.triu(matrix, 1)\n",
    "        ops_ind = np.random.choice(NASBenchConstants.OPS_IND, size=NASBenchConstants.NUM_VERTICES-2)\n",
    "        ops_onehot = np.concatenate([SpecOneHot.onehot_lu[i] for i in ops_ind])\n",
    "        flat = SpecOneHot.encode(matrix,ops_onehot)\n",
    "        m,o = SpecOneHot.decode(flat)\n",
    "        assert np.array_equal(matrix,m) \n",
    "        assert np.array_equal(ops_ind,o) \n",
    "\n",
    "    @staticmethod\n",
    "    def spec_from_flat(flat):\n",
    "        matrix,indices = SpecOneHot.decode(flat)\n",
    "        ops = np.asarray(NASBenchConstants.ALLOWED_OPS)[indices]\n",
    "        cops = np.concatenate(([NASBenchConstants.INPUT], ops, [NASBenchConstants.OUTPUT])).tolist()      \n",
    "        spec = SpecOneHot(matrix=matrix, ops=cops)\n",
    "        return spec\n",
    "    \n",
    "# this returns a random spec from the NAS-Bench-101    \n",
    "def random_spec(nasbench: NASBench):\n",
    "    \"\"\"Returns a random valid spec.\"\"\"\n",
    "    while True:\n",
    "        matrix = np.random.choice(NASBenchConstants.ALLOWED_EDGES,\n",
    "                                  size=(NASBenchConstants.NUM_VERTICES, NASBenchConstants.NUM_VERTICES))\n",
    "        matrix = np.triu(matrix, 1)\n",
    "        ops = np.random.choice(NASBenchConstants.ALLOWED_OPS, size=NASBenchConstants.NUM_VERTICES).tolist()\n",
    "        ops[0] = NASBenchConstants.INPUT\n",
    "        ops[-1] = NASBenchConstants.OUTPUT\n",
    "        spec = SpecOneHot(matrix=matrix, ops=ops)\n",
    "        if nasbench.is_valid(spec):\n",
    "            return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we define regularized evolution search as in https://github.com/google-research/nasbench/blob/b94247037ee470418a3e56dcb83814e9be83f3a8/NASBench.ipynb#L339\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "def random_combination(iterable, sample_size):\n",
    "    \"\"\"Random selection from itertools.combinations(iterable, r).\"\"\"\n",
    "    pool = tuple(iterable)\n",
    "    n = len(pool)\n",
    "    indices = sorted(random.sample(range(n), sample_size))\n",
    "    return tuple(pool[i] for i in indices)\n",
    "\n",
    "def mutate_spec(old_spec, nasbench: NASBench, mutation_rate=1.0):\n",
    "    \"\"\"Computes a valid mutated spec from the old_spec.\"\"\"\n",
    "    while True:\n",
    "        new_matrix = copy.deepcopy(old_spec.original_matrix)\n",
    "        new_ops = copy.deepcopy(old_spec.original_ops)\n",
    "\n",
    "        # In expectation, V edges flipped (note that most end up being pruned).\n",
    "        edge_mutation_prob = mutation_rate / NASBenchConstants.NUM_VERTICES\n",
    "        for src in range(0, NASBenchConstants.NUM_VERTICES - 1):\n",
    "            for dst in range(src + 1, NASBenchConstants.NUM_VERTICES):\n",
    "                if random.random() < edge_mutation_prob:\n",
    "                    new_matrix[src, dst] = 1 - new_matrix[src, dst]\n",
    "\n",
    "        # In expectation, one op is resampled.\n",
    "        op_mutation_prob = mutation_rate / NASBenchConstants.OP_SPOTS\n",
    "        for ind in range(1, NASBenchConstants.NUM_VERTICES - 1):\n",
    "            if random.random() < op_mutation_prob:\n",
    "                available = [\n",
    "                    o for o in nasbench.config[\"available_ops\"]\n",
    "                    if o != new_ops[ind]\n",
    "                ]\n",
    "                new_ops[ind] = random.choice(available)\n",
    "\n",
    "        new_spec = SpecOneHot(new_matrix, new_ops)\n",
    "        if nasbench.is_valid(new_spec):\n",
    "            return new_spec\n",
    "\n",
    "def run_revolution_search(\n",
    "    nasbench: NASBench,\n",
    "    max_time_budget=5e6,\n",
    "    population_size=50,\n",
    "    tournament_size=10,\n",
    "    mutation_rate=0.5\n",
    "):\n",
    "    \"\"\"Run a single roll-out of regularized evolution to a fixed time budget.\"\"\"\n",
    "\n",
    "    times, best_valids, best_tests = [0.0], [0.0], [0.0]\n",
    "    population = []  # (validation, spec) tuples\n",
    "\n",
    "    # For the first population_size individuals, seed the population with\n",
    "    # randomly generated cells.\n",
    "    for _ in range(population_size):\n",
    "        spec = random_spec(nasbench)\n",
    "        data = nasbench.query(spec)\n",
    "        time_spent, _ = nasbench.get_budget_counters()\n",
    "        times.append(time_spent)\n",
    "        population.append((data[\"validation_accuracy\"], spec))\n",
    "\n",
    "        if data[\"validation_accuracy\"] > best_valids[-1]:\n",
    "            best_valids.append(data[\"validation_accuracy\"])\n",
    "            best_tests.append(data[\"test_accuracy\"])\n",
    "        else:\n",
    "            best_valids.append(best_valids[-1])\n",
    "            best_tests.append(best_tests[-1])\n",
    "\n",
    "        if time_spent > max_time_budget:\n",
    "            break\n",
    "    # After the population is seeded, proceed with evolving the population.\n",
    "    while True:\n",
    "        sample = random_combination(population, tournament_size)\n",
    "        best_spec = sorted(sample, key=lambda i: i[0])[-1][1]\n",
    "        new_spec = mutate_spec(best_spec, nasbench, mutation_rate)\n",
    "\n",
    "        data = nasbench.query(new_spec)\n",
    "        time_spent, _ = nasbench.get_budget_counters()\n",
    "        times.append(time_spent)\n",
    "\n",
    "        # In regularized evolution, we kill the oldest individual.\n",
    "        population.append((data[\"validation_accuracy\"], new_spec))\n",
    "        population.pop(0)\n",
    "\n",
    "        if data[\"validation_accuracy\"] > best_valids[-1]:\n",
    "            best_valids.append(data[\"validation_accuracy\"])\n",
    "            best_tests.append(data[\"test_accuracy\"])\n",
    "        else:\n",
    "            best_valids.append(best_valids[-1])\n",
    "            best_tests.append(best_tests[-1])\n",
    "\n",
    "        if time_spent > max_time_budget:\n",
    "            break\n",
    "\n",
    "    return times, best_valids, best_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we define plotting functions so we can define experiments and plot the data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from IPython.display import clear_output\n",
    "from scipy.stats import t # for statistical testing\n",
    "\n",
    "def plot_run(data, color, label, config, ax=None, gran=10000):\n",
    "  \"\"\"Computes the mean and std\"\"\"\n",
    "\n",
    "  # which = 2 is test, which = 1 is valid\n",
    "  which = 2\n",
    "  if config[\"dataset\"] == \"test\":\n",
    "    which = 2\n",
    "  elif config[\"dataset\"] == \"validation\": \n",
    "    which = 1\n",
    "  xs = range(0, config[\"budget\"]+1, gran)\n",
    "  mean = [0.0]\n",
    "  std = [0.0]\n",
    "  per25 = [0.0]\n",
    "  per75 = [0.0]\n",
    "  ci_lower = [0.0]\n",
    "  ci_upper = [0.0]\n",
    "  repeats = len(data)\n",
    "  pointers = [1 for _ in range(repeats)]\n",
    "  \n",
    "  cur = gran\n",
    "  while cur < config[\"budget\"]+1:\n",
    "    all_vals = []\n",
    "    for repeat in range(repeats):\n",
    "      while (pointers[repeat] < len(data[repeat][0]) and \n",
    "             data[repeat][0][pointers[repeat]] < cur):\n",
    "        pointers[repeat] += 1\n",
    "      prev_time = data[repeat][0][pointers[repeat]-1]\n",
    "      prev_test = data[repeat][which][pointers[repeat]-1]\n",
    "      next_time = data[repeat][0][pointers[repeat]]\n",
    "      next_test = data[repeat][which][pointers[repeat]]\n",
    "      assert prev_time < cur and next_time >= cur\n",
    "\n",
    "      # Linearly interpolate the test between the two surrounding points\n",
    "      cur_val = ((cur - prev_time) / (next_time - prev_time)) * (next_test - prev_test) + prev_test\n",
    "      \n",
    "      all_vals.append(cur_val)\n",
    "      \n",
    "    all_vals = sorted(all_vals)\n",
    "\n",
    "\n",
    "    all_vals = sorted(all_vals)\n",
    "    cur_mean = sum(all_vals) / float(len(all_vals))\n",
    "    cur_std = np.std(all_vals)\n",
    "    std.append(cur_std)\n",
    "    \n",
    "    mean.append(sum(all_vals) / float(len(all_vals)))\n",
    "    per25.append(all_vals[int(0.25 * repeats)])\n",
    "    per75.append(all_vals[int(0.75 * repeats)])\n",
    "\n",
    "    if config[\"confidence_intervall\"] is True:\n",
    "\n",
    "      # Calculate the confidence interval\n",
    "      sem = cur_std / np.sqrt(repeats)  # Standard Error\n",
    "      confidence = 1 - config[\"pvalue\"]\n",
    "      t_critical = t.ppf(confidence + (1 - confidence) / 2, repeats - 1)\n",
    "      margin_of_error = t_critical * sem\n",
    "        \n",
    "      ci_lower.append(cur_mean - margin_of_error)\n",
    "      ci_upper.append(cur_mean + margin_of_error)\n",
    "    \n",
    "    cur += gran\n",
    "\n",
    "  if config[\"confidence_intervall\"] is True:\n",
    "    ax.fill_between(xs, ci_lower, ci_upper, alpha=0.5, linewidth=0, facecolor=color)\n",
    "  ax.plot(xs, mean, color=color, label=label, linewidth=2)\n",
    "  # plt.fill_between(xs, per25, per75, alpha=0.1, linewidth=0, facecolor=color)\n",
    "\n",
    "  return mean, std\n",
    "\n",
    "\n",
    "def plot_significance(means, stds, n, red, green, ax, pvalue_threshold):\n",
    "    comparisons = len(means)\n",
    "\n",
    "    all_pvalues = []\n",
    "    for a in range(comparisons):\n",
    "        for b in range(comparisons):\n",
    "\n",
    "            if a <=b :\n",
    "                continue\n",
    "            pvalues = []\n",
    "            for mean1, std1, mean2, std2 in zip(means[a], stds[a], means[b], stds[b]):\n",
    "                \n",
    "                se1 = std1 / np.sqrt(n)\n",
    "                se2 = std2 / np.sqrt(n)\n",
    "\n",
    "                if se1==0 and se2==0:\n",
    "                    pvalues.append(0.0)\n",
    "                    continue\n",
    "                t_stat = (mean1 - mean2) / np.sqrt(se1**2 + se2**2)\n",
    "                \n",
    "                df = ((se1**2 + se2**2)**2) / (((se1**2)**2 / (n-1)) + ((se2**2)**2 / (n-1)))\n",
    "                \n",
    "                p_value = 2 * t.sf(np.abs(t_stat), df)\n",
    "                pvalues.append(p_value)\n",
    "            # plt.plot(range(len(pvalues)), pvalues, label=f\"comparing {b} and {a}\", linewidth=2)\n",
    "            all_pvalues.append(pvalues)\n",
    "    # plt.show()\n",
    "    for idx in range(len(all_pvalues[0])):\n",
    "        color = green\n",
    "        for k in range(len(all_pvalues)):\n",
    "            p = all_pvalues[k][idx]\n",
    "            if p > pvalue_threshold: # at least one red\n",
    "                color = red\n",
    "\n",
    "        if color == green:\n",
    "            continue\n",
    "        ax.vlines(x=idx*10000, ymin=0.9, ymax=0.943, alpha=0.1,color=color, linewidth=2)\n",
    "\n",
    "    # this line is just for plotting the legend label once\n",
    "    ax.vlines(x=idx*10000, ymin=1, ymax=1.1,color=red, linewidth=2, label=f\"pvalue > {pvalue_threshold}\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_all(exp, ax=None):\n",
    "    # tableau_colorblind10\n",
    "    tc = {\n",
    "        'Dark Blue': '#006BA4',\n",
    "        'Blue': '#5F9ED1',\n",
    "        'Light Blue': '#A2C8EC',\n",
    "        'Dark Orange': '#FF800E',\n",
    "        'Red Orange': '#C85200',\n",
    "        'Light Orange': '#FFBC79',\n",
    "        'Very Dark Gray': '#595959',\n",
    "        'Dark Gray': '#898989',\n",
    "        'Light Gray': '#ABABAB',\n",
    "        'Very Light Gray': '#CFCFCF',\n",
    "        \"Green\" : '#228B22',\n",
    "        \"Red\" : '#FF4500' \n",
    "    }\n",
    "\n",
    "    # clear_output(wait=True)  # Clear the previous output\n",
    "\n",
    "    means, stds = [],[]\n",
    "    for key,value in exp[\"data\"].items():\n",
    "        label = key\n",
    "        data = value[0]\n",
    "        color = value[1]\n",
    "\n",
    "        mean, std = plot_run(data, tc[color], label, exp[\"config\"], ax)\n",
    "        means.append(mean)\n",
    "        stds.append(std)\n",
    "\n",
    "\n",
    "    if exp[\"config\"][\"significant_areas\"]:\n",
    "        plot_significance(means, stds, exp[\"config\"][\"n\"], tc['Light Gray'], tc['Green'], ax, exp[\"config\"][\"pvalue\"])\n",
    "\n",
    "\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_ylim(exp[\"config\"][\"limits\"][0], exp[\"config\"][\"limits\"][1])\n",
    "    ax.set_xlabel('total training time spent (seconds)')\n",
    "    ax.set_ylabel('accuracy')\n",
    "    return means, stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can go ahead with a run.\n",
    "\n",
    "# we can initialize an experiment like this, feel free to modify configs\n",
    "exp1 = {\n",
    "    \"data\": \n",
    "    {\n",
    "        \"regularized evolution\" : [[],\"Very Dark Gray\"],\n",
    "\n",
    "    },\n",
    "    \"config\": \n",
    "    {\n",
    "        \"budget\" : int(5e5), # maximum time budget for the experiment\n",
    "        \"limits\" : (0.93, 0.9435), # y limits for the plot\n",
    "        \"n\" : 10, # number of runs\n",
    "        \"print_every\" : 2, # update you view ever n runs\n",
    "        \"confidence_intervall\" : True, # plotting the confidence intervall\n",
    "        \"pvalue\" : 0.05, # pvalue when comparing multiple algorithms\n",
    "        \"significant_areas\": False, # plotting significant areas with multiple algorithms\n",
    "        \"dataset\" : \"test\", # which dataset to use for evaluation\n",
    "   }\n",
    "}\n",
    "\n",
    "budget = exp1[\"config\"][\"budget\"] # get budget from config\n",
    "for run in range(exp1[\"config\"][\"n\"]):\n",
    "    nasb.reset_budget_counters() # we need to rest this before every search run\n",
    "    times, best_valid, best_test = run_revolution_search(nasb, budget, 50,10,0.72)\n",
    "    exp1[\"data\"][\"regularized evolution\"][0].append((times, best_valid, best_test))\n",
    "\n",
    "    if (run % exp1[\"config\"][\"print_every\"] == 0): # plot intermediate results\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots()\n",
    "        plot_all(exp1, ax)\n",
    "        plt.show()\n",
    "        print('Running repeat %d' % (run + 1))\n",
    "\n",
    "# plot final result\n",
    "clear_output(wait=True)\n",
    "fig, ax = plt.subplots()\n",
    "plot_all(exp1, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we will look into the NATS-Bench\n",
    "from collections import OrderedDict\n",
    "from nats_bench import create\n",
    "from scipy.stats import t\n",
    "\n",
    "# as this library saves its data in its individual files, we will only use \n",
    "# scripts to access and read out the results and then visualize them. \n",
    "\n",
    "\n",
    "# we expect to fetch data from ../output/search/tss\n",
    "def fetch_data(root_dir=None, search_space=\"tss\", dataset=None, algorithms=[\"REA\",\"REINFORCE\", \"RANDOM\", \"GM\", \"GE\", \"BOHB\"]):\n",
    "    if root_dir is None:\n",
    "        root_dir = os.path.join(\"..\", \"output\", \"search\")\n",
    "    ss_dir = \"{:}-{:}\".format(root_dir, search_space)\n",
    "    alg2name, alg2path = OrderedDict(), OrderedDict()\n",
    "    for alg in algorithms:\n",
    "        alg2name[alg] = alg\n",
    "    for alg, name in alg2name.items():\n",
    "        alg2path[alg] = os.path.join(ss_dir, dataset, name, \"results.pth\")\n",
    "        assert os.path.isfile(alg2path[alg]), \"invalid path : {:}\".format(alg2path[alg])\n",
    "    alg2data = OrderedDict()\n",
    "    for alg, path in alg2path.items():\n",
    "        data = torch.load(path)\n",
    "        for index, info in data.items():\n",
    "            info[\"time_w_arch\"] = [\n",
    "                (x, y) for x, y in zip(info[\"all_total_times\"], info[\"all_archs\"])\n",
    "            ]\n",
    "            for j, arch in enumerate(info[\"all_archs\"]):\n",
    "                assert arch != -1, \"invalid arch from {:} {:} {:} ({:}, {:})\".format(\n",
    "                    alg, search_space, dataset, index, j\n",
    "                )\n",
    "        alg2data[alg] = data\n",
    "    return alg2data\n",
    "\n",
    "\n",
    "def query_performance(api, data, dataset, ticket):\n",
    "    results, is_size_space = [], api.search_space_name == \"size\"\n",
    "    for i, info in data.items():\n",
    "        time_w_arch = sorted(info[\"time_w_arch\"], key=lambda x: abs(x[0] - ticket))\n",
    "        time_a, arch_a = time_w_arch[0]\n",
    "        time_b, arch_b = time_w_arch[1]\n",
    "        info_a = api.get_more_info(\n",
    "            arch_a, dataset=dataset, hp=90 if is_size_space else 200, is_random=False\n",
    "        )\n",
    "        info_b = api.get_more_info(\n",
    "            arch_b, dataset=dataset, hp=90 if is_size_space else 200, is_random=False\n",
    "        )\n",
    "        accuracy_a, accuracy_b = info_a[\"test-accuracy\"], info_b[\"test-accuracy\"]\n",
    "        interplate = (time_b - ticket) / (time_b - time_a) * accuracy_a + (\n",
    "            ticket - time_a\n",
    "        ) / (time_b - time_a) * accuracy_b\n",
    "        results.append(interplate)\n",
    "    return np.mean(results), np.std(results)\n",
    "\n",
    "\n",
    "def show_valid_test(api, data, dataset):\n",
    "    valid_accs, test_accs, is_size_space = [], [], api.search_space_name == \"size\"\n",
    "    for i, info in data.items():\n",
    "        time, arch = info[\"time_w_arch\"][-1]\n",
    "        if dataset == \"cifar10\":\n",
    "            xinfo = api.get_more_info(\n",
    "                arch, dataset=dataset, hp=90 if is_size_space else 200, is_random=False\n",
    "            )\n",
    "            test_accs.append(xinfo[\"test-accuracy\"])\n",
    "            xinfo = api.get_more_info(\n",
    "                arch,\n",
    "                dataset=\"cifar10-valid\",\n",
    "                hp=90 if is_size_space else 200,\n",
    "                is_random=False,\n",
    "            )\n",
    "            valid_accs.append(xinfo[\"valid-accuracy\"])\n",
    "        else:\n",
    "            xinfo = api.get_more_info(\n",
    "                arch, dataset=dataset, hp=90 if is_size_space else 200, is_random=False\n",
    "            )\n",
    "            valid_accs.append(xinfo[\"valid-accuracy\"])\n",
    "            test_accs.append(xinfo[\"test-accuracy\"])\n",
    "    valid_str = \"{:.2f}$\\pm${:.2f}\".format(np.mean(valid_accs), np.std(valid_accs))\n",
    "    test_str = \"{:.2f}$\\pm${:.2f}\".format(np.mean(test_accs), np.std(test_accs))\n",
    "    return valid_str, test_str\n",
    "\n",
    "\n",
    "ylims ={\n",
    "    \"tss\" : {\n",
    "        \"cifar10\": (91,94.3),\n",
    "        \"cifar100\": (69,75.0),\n",
    "        \"ImageNet16-120\": (42,52)\n",
    "    },\n",
    "    \"sss\" : {\n",
    "        \"cifar10\": (92,93.3),\n",
    "        \"cifar100\": (65,70.5),\n",
    "        \"ImageNet16-120\": (40,46)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def visualize_curve(api, exp, dataset, ax, search_space=\"tss\"):\n",
    "    def sub_plot_fn(ax, dataset, exp):\n",
    "        xdataset, max_time = dataset.split(\"-T\")\n",
    "        algorithms_labels = [d[0] for d in exp[\"data\"].items()]\n",
    "        algorithms = [d[1][0] for d in exp[\"data\"].items()]\n",
    "        alg2data = fetch_data(search_space=search_space, dataset=dataset, algorithms=algorithms)\n",
    "        total_tickets = 150\n",
    "        time_tickets = [\n",
    "            float(i) / total_tickets * int(max_time) for i in range(total_tickets)\n",
    "        ]\n",
    "        tc = {\n",
    "            'Dark Blue': '#006BA4',\n",
    "            'Blue': '#5F9ED1',\n",
    "            'Light Blue': '#A2C8EC',\n",
    "            'Dark Orange': '#FF800E',\n",
    "            'Red Orange': '#C85200',\n",
    "            'Light Orange': '#FFBC79',\n",
    "            'Very Dark Gray': '#595959',\n",
    "            'Dark Gray': '#898989',\n",
    "            'Light Gray': '#ABABAB',\n",
    "            'Very Light Gray': '#CFCFCF',\n",
    "            \"Green\" : '#228B22',\n",
    "            \"Red\" : '#FF4500' \n",
    "        }\n",
    "        colors = [tc[d[1][1]] for d in exp[\"data\"].items()]\n",
    "\n",
    "        # ax.set_xlim(0,exp[\"config\"][\"budget\"]) # to make it easier for different experiments, we encode budget in name\n",
    "        ax.set_xlim(0,int(max_time))\n",
    "        \n",
    "        ax.set_ylim(\n",
    "            exp[\"config\"][\"limits\"][xdataset][0], exp[\"config\"][\"limits\"][xdataset][1]\n",
    "        )\n",
    "\n",
    "        xs = [x for x in time_tickets]\n",
    "\n",
    "        for idx, (alg, data) in enumerate(alg2data.items()):\n",
    "            accuracies = []\n",
    "            ci_lower = []\n",
    "            ci_upper = []\n",
    "            repeats = len(data)\n",
    "            for ticket in time_tickets:\n",
    "                cur_mean, cur_std = query_performance(api, data, xdataset, ticket)\n",
    "                accuracies.append(cur_mean)\n",
    "                if exp[\"config\"][\"confidence_intervall\"] is True:\n",
    "\n",
    "                    # Calculate the confidence interval\n",
    "                    sem = cur_std / np.sqrt(repeats)  # Standard Error\n",
    "                    confidence = 1 - exp[\"config\"][\"pvalue\"]\n",
    "                    t_critical = t.ppf(confidence + (1 - confidence) / 2, repeats - 1)\n",
    "                    margin_of_error = t_critical * sem\n",
    "                        \n",
    "                    ci_lower.append(cur_mean - margin_of_error)\n",
    "                    ci_upper.append(cur_mean + margin_of_error)\n",
    "                    \n",
    "            ax.plot(\n",
    "                xs,\n",
    "                accuracies,\n",
    "                c=colors[idx],\n",
    "                label=\"{:}\".format(algorithms_labels[idx]),\n",
    "            )\n",
    "            if exp[\"config\"][\"confidence_intervall\"] is True:\n",
    "                ax.fill_between(xs, ci_lower, ci_upper, alpha=0.5, linewidth=0, facecolor=colors[idx])\n",
    "\n",
    "            ax.set_ylabel(\"accuracy\")\n",
    "            ax.set_xlabel('total training time spent (seconds)')\n",
    "\n",
    "            name2label = {\n",
    "                \"cifar10\": \"CIFAR-10\",\n",
    "                \"cifar100\": \"CIFAR-100\",\n",
    "                \"ImageNet16-120\": \"ImageNet-16-120\",\n",
    "            }\n",
    "            ax.set_title(\"NATS-Bench results on {:}\".format(name2label[xdataset]))\n",
    "            formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "            formatter.set_powerlimits((1,4))\n",
    "            ax.xaxis.set_major_formatter(formatter)\n",
    "            # ax.xaxis.set_major_locator(ticker.MultipleLocator(100))\n",
    "\n",
    "\n",
    "        ax.legend(loc=4)\n",
    "    sub_plot_fn(ax, dataset, exp)\n",
    "    print(\"sub-plot {:} on {:} done.\".format(dataset, search_space))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can start define experiments from the provided script in our autodl submodule\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def run_algorithm(algname, dataset, budget, loops): \n",
    "    algorithms = {\n",
    "\n",
    "        # custom scripts like this\n",
    "        # \"custom evolution\": os.path.join(\"custom_evolution.py\"),\n",
    "        \"GENE\": os.path.join(\"4_GENE.py\"),\n",
    "\n",
    "        # baselines provided from autodl lib\n",
    "        \"regularized evolution\": os.path.join(\"..\", \"thirdparty\", \"autodl\", \"exps\", \"NATS-algos\", \"regularized_ea.py\"),\n",
    "        \"random\": os.path.join(\"..\", \"thirdparty\", \"autodl\", \"exps\", \"NATS-algos\", \"random_wo_share.py\"),\n",
    "        \"reinforce\": os.path.join(\"..\", \"thirdparty\", \"autodl\", \"exps\", \"NATS-algos\", \"reinforce.py\"),\n",
    "        \"bohb\": os.path.join(\"..\", \"thirdparty\", \"autodl\", \"exps\", \"NATS-algos\", \"bohb.py\"),\n",
    "        \n",
    "    }\n",
    "    # Use os.path.join to construct the save_dir\n",
    "    save_dir = os.path.join(\"..\", \"output\", \"search\")\n",
    "\n",
    "    print(f\"Running algorithm {algname} on {dataset}...\")\n",
    "    command = [\n",
    "        \"python\", \n",
    "        algorithms[algname],\n",
    "        \"--save_dir\", save_dir, \n",
    "        \"--dataset\", dataset,\n",
    "        \"--search_space\", \"tss\",\n",
    "        \"--time_budget\", str(budget),\n",
    "        \"--loops_if_rand\", str(loops),\n",
    "\n",
    "    ]\n",
    "\n",
    "    if algname== \"regularized evolution\" or algname== \"regularized evolution gm\": \n",
    "        command += [\"--ea_cycles\", \"200\",\"--ea_population\", \"20\",\"--ea_sample_size\", \"10\"]\n",
    "\n",
    "    if algname==\"bohb\":\n",
    "        command += [\"--num_samples\", \"4\", \"--random_fraction\", \"0.0\", \"--bandwidth_factor\",\"3\"]\n",
    "\n",
    "    if algname==\"reinforce\":\n",
    "        command += [\"--learning_rate\", \"0.01\"]\n",
    "\n",
    "\n",
    "    # Run the command\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "    # if no outputs are saved, check this command\n",
    "    # print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how we run the algorithms from nats-bench\n",
    "\n",
    "algorithms = [\n",
    "    \"regularized evolution\",\n",
    "    \"random\",\n",
    "    # \"reinforce\",\n",
    "\n",
    "]\n",
    "datasets = {\n",
    "    \"cifar10\" : 2000, # time budget\n",
    "    # \"cifar100\" : 400000,\n",
    "    # \"ImageNet16-120\": 120000,\n",
    "}\n",
    "\n",
    "n = 5 # number of runs\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    for dataset, budget in datasets.items():\n",
    "        run_algorithm(algorithm, dataset, budget, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can visualize our experiments\n",
    "\n",
    "search_space = \"tss\"\n",
    "api = create(None, search_space, fast_mode=True, verbose=False)\n",
    "\n",
    "budget = int(2000) # this has to match with the experiment from above\n",
    "datasets = [f\"cifar10-T{budget}\"]\n",
    "ylims = {\n",
    "        \"cifar10\": (91,95.3),\n",
    "        \"cifar100\": (69,75.0),\n",
    "        \"ImageNet16-120\": (42,52)}\n",
    "\n",
    "\n",
    "exp1 = {\n",
    "    \"data\": \n",
    "    {\n",
    "        \"regularized evolution\" : [\"R-EA-SS10\",\"Dark Blue\"],\n",
    "        # \"reinforce\" : [\"REINFORCE-0.01\",\"Light Gray\"],\n",
    "        \"random\" : [\"RANDOM\",\"Red Orange\"],\n",
    "        # \"bohb\" : [\"BOHB\",\"Dark Orange\"],\n",
    "\n",
    "    },\n",
    "    \"config\": \n",
    "    {\n",
    "        \"budget\" : int(2e4),\n",
    "        \"limits\" : ylims,\n",
    "        \"n\" : 1000,\n",
    "        \"print_every\" : 5,\n",
    "        \"confidence_intervall\" : True,\n",
    "        \"pvalue\" : 0.05,\n",
    "        \"significant_areas\": False,\n",
    "        \"dataset\" : \"test\",\n",
    "   }\n",
    "}\n",
    "\n",
    "# one plot\n",
    "fig, ax = plt.subplots()\n",
    "visualize_curve(api, exp1, datasets[0], ax)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
